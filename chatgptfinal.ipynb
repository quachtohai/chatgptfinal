{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "671863d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-03T05:13:56.584122Z",
     "iopub.status.busy": "2024-02-03T05:13:56.583264Z",
     "iopub.status.idle": "2024-02-03T05:14:04.545304Z",
     "shell.execute_reply": "2024-02-03T05:14:04.544151Z"
    },
    "papermill": {
     "duration": 7.972993,
     "end_time": "2024-02-03T05:14:04.547388",
     "exception": false,
     "start_time": "2024-02-03T05:13:56.574395",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchtext\n",
    "import nltk\n",
    "\n",
    "import unicodedata\n",
    "import os\n",
    "import string\n",
    "import re\n",
    "\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "USE_GPU = True\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print('using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a15f693",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-03T05:14:04.561041Z",
     "iopub.status.busy": "2024-02-03T05:14:04.560599Z",
     "iopub.status.idle": "2024-02-03T05:14:04.571628Z",
     "shell.execute_reply": "2024-02-03T05:14:04.570870Z"
    },
    "papermill": {
     "duration": 0.020142,
     "end_time": "2024-02-03T05:14:04.573681",
     "exception": false,
     "start_time": "2024-02-03T05:14:04.553539",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "      if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "\n",
    "def preprocess_sentence(text):\n",
    "    text = unicode_to_ascii(text.lower().strip())\n",
    "    text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = re.sub(r\"\\r\", \"\", text)\n",
    "    text = re.sub(r\"he's\", \"he is\", text)\n",
    "    text = re.sub(r\"she's\", \"she is\", text)\n",
    "    text = re.sub(r\"it's\", \"it is\", text)\n",
    "    text = re.sub(r\"that's\", \"that is\", text)\n",
    "    text = re.sub(r\"what's\", \"that is\", text)\n",
    "    text = re.sub(r\"where's\", \"where is\", text)\n",
    "    text = re.sub(r\"how's\", \"how is\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"can't\", \"cannot\", text)\n",
    "    text = re.sub(r\"n't\", \" not\", text)\n",
    "    text = re.sub(r\"n'\", \"ng\", text)\n",
    "    text = re.sub(r\"'bout\", \"about\", text)\n",
    "    text = re.sub(r\"'til\", \"until\", text)\n",
    "    text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", text)\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = re.sub(\"(\\\\W)\",\" \",text)\n",
    "    text = re.sub('\\S*\\d\\S*\\s*','', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ce9895a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-03T05:14:04.586748Z",
     "iopub.status.busy": "2024-02-03T05:14:04.586470Z",
     "iopub.status.idle": "2024-02-03T05:14:05.013719Z",
     "shell.execute_reply": "2024-02-03T05:14:05.012906Z"
    },
    "papermill": {
     "duration": 0.436347,
     "end_time": "2024-02-03T05:14:05.016056",
     "exception": false,
     "start_time": "2024-02-03T05:14:04.579709",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "lines = open('/kaggle/input/chatbot-data/cornell movie-dialogs corpus/movie_lines.txt', encoding='utf-8',\n",
    "             errors='ignore').read().split('\\n')\n",
    "\n",
    "conversatino = open('/kaggle/input/chatbot-data/cornell movie-dialogs corpus/movie_conversations.txt', encoding='utf-8',\n",
    "             errors='ignore').read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdebfe10",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-03T05:14:05.029701Z",
     "iopub.status.busy": "2024-02-03T05:14:05.029356Z",
     "iopub.status.idle": "2024-02-03T05:14:07.756148Z",
     "shell.execute_reply": "2024-02-03T05:14:07.755198Z"
    },
    "papermill": {
     "duration": 2.736402,
     "end_time": "2024-02-03T05:14:07.758635",
     "exception": false,
     "start_time": "2024-02-03T05:14:05.022233",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QUESTION</th>\n",
       "      <th>ANSWER</th>\n",
       "      <th>QUESTION_CLEAN</th>\n",
       "      <th>ANSWER_CLEAN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Can we make this quick?  Roxanne Korrine and A...</td>\n",
       "      <td>Well, I thought we'd start with pronunciation,...</td>\n",
       "      <td>can we make this quick  roxanne korrine and an...</td>\n",
       "      <td>well i thought we would start with pronunciati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Well, I thought we'd start with pronunciation,...</td>\n",
       "      <td>Not the hacking and gagging and spitting part....</td>\n",
       "      <td>well i thought we would start with pronunciati...</td>\n",
       "      <td>not the hacking and gagging and spitting part ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Not the hacking and gagging and spitting part....</td>\n",
       "      <td>Okay... then how 'bout we try out some French ...</td>\n",
       "      <td>not the hacking and gagging and spitting part ...</td>\n",
       "      <td>okay then how about we try out some french cui...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The thing is, Cameron -- I'm at the mercy of a...</td>\n",
       "      <td>Seems like she could get a date easy enough...</td>\n",
       "      <td>the thing is cameron  i am at the mercy of a p...</td>\n",
       "      <td>seems like she could get a date easy enough</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gosh, if only we could find Kat a boyfriend...</td>\n",
       "      <td>Let me see what I can do.</td>\n",
       "      <td>gosh if only we could find kat a boyfriend</td>\n",
       "      <td>let me see what i can do</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>We're not going that route.  Sonrisa's not som...</td>\n",
       "      <td>What else can we do?</td>\n",
       "      <td>we are not going that route  sonrisas not some...</td>\n",
       "      <td>what else can we do</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>What else can we do?</td>\n",
       "      <td>Like I said-- This isn't my regular line of wo...</td>\n",
       "      <td>what else can we do</td>\n",
       "      <td>like i said this is not my regular line of wor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>Goodnight Bears.  Goodnight chairs. Goodnight ...</td>\n",
       "      <td>I don't want to move away.</td>\n",
       "      <td>goodnight bears  goodnight chairs goodnight ki...</td>\n",
       "      <td>i do not want to move away</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>I don't want to move away.</td>\n",
       "      <td>I know, Honey, but just think how exiting it w...</td>\n",
       "      <td>i do not want to move away</td>\n",
       "      <td>i know honey but just think how exiting it wil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>I know, Honey, but just think how exiting it w...</td>\n",
       "      <td>But why can't we stay here?</td>\n",
       "      <td>i know honey but just think how exiting it wil...</td>\n",
       "      <td>but why cannot we stay here</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               QUESTION  \\\n",
       "0     Can we make this quick?  Roxanne Korrine and A...   \n",
       "1     Well, I thought we'd start with pronunciation,...   \n",
       "2     Not the hacking and gagging and spitting part....   \n",
       "3     The thing is, Cameron -- I'm at the mercy of a...   \n",
       "4        Gosh, if only we could find Kat a boyfriend...   \n",
       "...                                                 ...   \n",
       "9995  We're not going that route.  Sonrisa's not som...   \n",
       "9996                               What else can we do?   \n",
       "9997  Goodnight Bears.  Goodnight chairs. Goodnight ...   \n",
       "9998                         I don't want to move away.   \n",
       "9999  I know, Honey, but just think how exiting it w...   \n",
       "\n",
       "                                                 ANSWER  \\\n",
       "0     Well, I thought we'd start with pronunciation,...   \n",
       "1     Not the hacking and gagging and spitting part....   \n",
       "2     Okay... then how 'bout we try out some French ...   \n",
       "3        Seems like she could get a date easy enough...   \n",
       "4                             Let me see what I can do.   \n",
       "...                                                 ...   \n",
       "9995                               What else can we do?   \n",
       "9996  Like I said-- This isn't my regular line of wo...   \n",
       "9997                         I don't want to move away.   \n",
       "9998  I know, Honey, but just think how exiting it w...   \n",
       "9999                        But why can't we stay here?   \n",
       "\n",
       "                                         QUESTION_CLEAN  \\\n",
       "0     can we make this quick  roxanne korrine and an...   \n",
       "1     well i thought we would start with pronunciati...   \n",
       "2     not the hacking and gagging and spitting part ...   \n",
       "3     the thing is cameron  i am at the mercy of a p...   \n",
       "4            gosh if only we could find kat a boyfriend   \n",
       "...                                                 ...   \n",
       "9995  we are not going that route  sonrisas not some...   \n",
       "9996                                what else can we do   \n",
       "9997  goodnight bears  goodnight chairs goodnight ki...   \n",
       "9998                         i do not want to move away   \n",
       "9999  i know honey but just think how exiting it wil...   \n",
       "\n",
       "                                           ANSWER_CLEAN  \n",
       "0     well i thought we would start with pronunciati...  \n",
       "1     not the hacking and gagging and spitting part ...  \n",
       "2     okay then how about we try out some french cui...  \n",
       "3           seems like she could get a date easy enough  \n",
       "4                              let me see what i can do  \n",
       "...                                                 ...  \n",
       "9995                                what else can we do  \n",
       "9996  like i said this is not my regular line of wor...  \n",
       "9997                         i do not want to move away  \n",
       "9998  i know honey but just think how exiting it wil...  \n",
       "9999                        but why cannot we stay here  \n",
       "\n",
       "[10000 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversatino_rel_line = []\n",
    "for row in conversatino:\n",
    "    line_ids = row.split(' +++$+++ ')[-1][1:-1].replace(\"'\", \"\").split(', ')\n",
    "    conversatino_rel_line.append(line_ids)\n",
    "\n",
    "dialog = {}\n",
    "for line in lines:\n",
    "    row = line.split(' +++$+++ ')\n",
    "    dialog[row[0]] = row[-1]\n",
    "\n",
    "\n",
    "questions = []\n",
    "answers = []\n",
    "for link_ids in conversatino_rel_line:\n",
    "    for i in range(len(link_ids) - 1):\n",
    "        question = dialog[link_ids[i]].strip()\n",
    "        answer = dialog[link_ids[i + 1]].strip()\n",
    "        \n",
    "        #  Take sentence with 4 and more words\n",
    "        if len(question.split()) > 3 and len(answer.split()) > 3:\n",
    "          questions.append(question)\n",
    "          answers.append(answer)\n",
    "\n",
    "df = pd.DataFrame({'QUESTION': questions[0:10000], 'ANSWER': answers[0:10000]})\n",
    "# df = pd.DataFrame({'QUESTION': questions, 'ANSWER': answers})\n",
    "\n",
    "df.dropna(axis=0, how='any', inplace=True)\n",
    "\n",
    "df['QUESTION_CLEAN'] = df['QUESTION'].apply(lambda x: preprocess_sentence(x))\n",
    "df['ANSWER_CLEAN'] = df['ANSWER'].apply(lambda x: preprocess_sentence(x))\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36e10fb5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-03T05:14:07.773254Z",
     "iopub.status.busy": "2024-02-03T05:14:07.772950Z",
     "iopub.status.idle": "2024-02-03T05:14:25.488023Z",
     "shell.execute_reply": "2024-02-03T05:14:25.487058Z"
    },
    "papermill": {
     "duration": 17.724382,
     "end_time": "2024-02-03T05:14:25.490165",
     "exception": false,
     "start_time": "2024-02-03T05:14:07.765783",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answers</th>\n",
       "      <th>answersTmp</th>\n",
       "      <th>QUESTION_CLEAN</th>\n",
       "      <th>ANSWER_CLEAN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56be85543aeaaa14008c9063</td>\n",
       "      <td>Beyoncé</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>When did Beyonce start becoming popular?</td>\n",
       "      <td>{'text': ['in the late 1990s'], 'answer_start'...</td>\n",
       "      <td>['in the late 1990s']</td>\n",
       "      <td>when did beyonce start becoming popular</td>\n",
       "      <td>in the late</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56be85543aeaaa14008c9065</td>\n",
       "      <td>Beyoncé</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>What areas did Beyonce compete in when she was...</td>\n",
       "      <td>{'text': ['singing and dancing'], 'answer_star...</td>\n",
       "      <td>['singing and dancing']</td>\n",
       "      <td>what areas did beyonce compete in when she was...</td>\n",
       "      <td>singing and dancing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>56be85543aeaaa14008c9066</td>\n",
       "      <td>Beyoncé</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>When did Beyonce leave Destiny's Child and bec...</td>\n",
       "      <td>{'text': ['2003'], 'answer_start': [526]}</td>\n",
       "      <td>['2003']</td>\n",
       "      <td>when did beyonce leave destinys child and beco...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56bf6b0f3aeaaa14008c9601</td>\n",
       "      <td>Beyoncé</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>In what city and state did Beyonce  grow up?</td>\n",
       "      <td>{'text': ['Houston, Texas'], 'answer_start': [...</td>\n",
       "      <td>['Houston, Texas']</td>\n",
       "      <td>in what city and state did beyonce  grow up</td>\n",
       "      <td>houston texas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56bf6b0f3aeaaa14008c9602</td>\n",
       "      <td>Beyoncé</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>In which decade did Beyonce become famous?</td>\n",
       "      <td>{'text': ['late 1990s'], 'answer_start': [276]}</td>\n",
       "      <td>['late 1990s']</td>\n",
       "      <td>in which decade did beyonce become famous</td>\n",
       "      <td>late</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130314</th>\n",
       "      <td>5a7e070b70df9f001a875439</td>\n",
       "      <td>Matter</td>\n",
       "      <td>The term \"matter\" is used throughout physics i...</td>\n",
       "      <td>Physics has broadly agreed on the definition o...</td>\n",
       "      <td>{'text': [], 'answer_start': []}</td>\n",
       "      <td>[]</td>\n",
       "      <td>physics has broadly agreed on the definition o...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130315</th>\n",
       "      <td>5a7e070b70df9f001a87543a</td>\n",
       "      <td>Matter</td>\n",
       "      <td>The term \"matter\" is used throughout physics i...</td>\n",
       "      <td>Who coined the term partonic matter?</td>\n",
       "      <td>{'text': [], 'answer_start': []}</td>\n",
       "      <td>[]</td>\n",
       "      <td>who coined the term partonic matter</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130316</th>\n",
       "      <td>5a7e070b70df9f001a87543b</td>\n",
       "      <td>Matter</td>\n",
       "      <td>The term \"matter\" is used throughout physics i...</td>\n",
       "      <td>What is another name for anti-matter?</td>\n",
       "      <td>{'text': [], 'answer_start': []}</td>\n",
       "      <td>[]</td>\n",
       "      <td>what is another name for antimatter</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130317</th>\n",
       "      <td>5a7e070b70df9f001a87543c</td>\n",
       "      <td>Matter</td>\n",
       "      <td>The term \"matter\" is used throughout physics i...</td>\n",
       "      <td>Matter usually does not need to be used in con...</td>\n",
       "      <td>{'text': [], 'answer_start': []}</td>\n",
       "      <td>[]</td>\n",
       "      <td>matter usually does not need to be used in con...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130318</th>\n",
       "      <td>5a7e070b70df9f001a87543d</td>\n",
       "      <td>Matter</td>\n",
       "      <td>The term \"matter\" is used throughout physics i...</td>\n",
       "      <td>What field of study has a variety of unusual c...</td>\n",
       "      <td>{'text': [], 'answer_start': []}</td>\n",
       "      <td>[]</td>\n",
       "      <td>what field of study has a variety of unusual c...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>130319 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              id    title  \\\n",
       "0       56be85543aeaaa14008c9063  Beyoncé   \n",
       "1       56be85543aeaaa14008c9065  Beyoncé   \n",
       "2       56be85543aeaaa14008c9066  Beyoncé   \n",
       "3       56bf6b0f3aeaaa14008c9601  Beyoncé   \n",
       "4       56bf6b0f3aeaaa14008c9602  Beyoncé   \n",
       "...                          ...      ...   \n",
       "130314  5a7e070b70df9f001a875439   Matter   \n",
       "130315  5a7e070b70df9f001a87543a   Matter   \n",
       "130316  5a7e070b70df9f001a87543b   Matter   \n",
       "130317  5a7e070b70df9f001a87543c   Matter   \n",
       "130318  5a7e070b70df9f001a87543d   Matter   \n",
       "\n",
       "                                                  context  \\\n",
       "0       Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
       "1       Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
       "2       Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
       "3       Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
       "4       Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
       "...                                                   ...   \n",
       "130314  The term \"matter\" is used throughout physics i...   \n",
       "130315  The term \"matter\" is used throughout physics i...   \n",
       "130316  The term \"matter\" is used throughout physics i...   \n",
       "130317  The term \"matter\" is used throughout physics i...   \n",
       "130318  The term \"matter\" is used throughout physics i...   \n",
       "\n",
       "                                                 question  \\\n",
       "0                When did Beyonce start becoming popular?   \n",
       "1       What areas did Beyonce compete in when she was...   \n",
       "2       When did Beyonce leave Destiny's Child and bec...   \n",
       "3           In what city and state did Beyonce  grow up?    \n",
       "4              In which decade did Beyonce become famous?   \n",
       "...                                                   ...   \n",
       "130314  Physics has broadly agreed on the definition o...   \n",
       "130315               Who coined the term partonic matter?   \n",
       "130316              What is another name for anti-matter?   \n",
       "130317  Matter usually does not need to be used in con...   \n",
       "130318  What field of study has a variety of unusual c...   \n",
       "\n",
       "                                                  answers  \\\n",
       "0       {'text': ['in the late 1990s'], 'answer_start'...   \n",
       "1       {'text': ['singing and dancing'], 'answer_star...   \n",
       "2               {'text': ['2003'], 'answer_start': [526]}   \n",
       "3       {'text': ['Houston, Texas'], 'answer_start': [...   \n",
       "4         {'text': ['late 1990s'], 'answer_start': [276]}   \n",
       "...                                                   ...   \n",
       "130314                   {'text': [], 'answer_start': []}   \n",
       "130315                   {'text': [], 'answer_start': []}   \n",
       "130316                   {'text': [], 'answer_start': []}   \n",
       "130317                   {'text': [], 'answer_start': []}   \n",
       "130318                   {'text': [], 'answer_start': []}   \n",
       "\n",
       "                     answersTmp  \\\n",
       "0         ['in the late 1990s']   \n",
       "1       ['singing and dancing']   \n",
       "2                      ['2003']   \n",
       "3            ['Houston, Texas']   \n",
       "4                ['late 1990s']   \n",
       "...                         ...   \n",
       "130314                       []   \n",
       "130315                       []   \n",
       "130316                       []   \n",
       "130317                       []   \n",
       "130318                       []   \n",
       "\n",
       "                                           QUESTION_CLEAN         ANSWER_CLEAN  \n",
       "0                 when did beyonce start becoming popular         in the late   \n",
       "1       what areas did beyonce compete in when she was...  singing and dancing  \n",
       "2       when did beyonce leave destinys child and beco...                       \n",
       "3             in what city and state did beyonce  grow up        houston texas  \n",
       "4               in which decade did beyonce become famous                late   \n",
       "...                                                   ...                  ...  \n",
       "130314  physics has broadly agreed on the definition o...                       \n",
       "130315                who coined the term partonic matter                       \n",
       "130316                what is another name for antimatter                       \n",
       "130317  matter usually does not need to be used in con...                       \n",
       "130318  what field of study has a variety of unusual c...                       \n",
       "\n",
       "[130319 rows x 8 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_parquet(\"/kaggle/input/squad-datasets/squad.parquet\")\n",
    "df.head()\n",
    "df[\"answers\"][0][\"text\"][0]\n",
    "df[\"answersTmp\"] = df[\"answers\"].apply(lambda x : str(x[\"text\"]))\n",
    "df.dropna(axis=0, how='any', inplace=True)\n",
    "df['QUESTION_CLEAN'] = df['question'].apply(lambda x: preprocess_sentence(x))\n",
    "df['ANSWER_CLEAN'] = df['answersTmp'].apply(lambda x: preprocess_sentence(x))\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed7fcb3f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-03T05:14:25.505263Z",
     "iopub.status.busy": "2024-02-03T05:14:25.504955Z",
     "iopub.status.idle": "2024-02-03T05:14:35.438182Z",
     "shell.execute_reply": "2024-02-03T05:14:35.437265Z"
    },
    "papermill": {
     "duration": 9.94348,
     "end_time": "2024-02-03T05:14:35.440524",
     "exception": false,
     "start_time": "2024-02-03T05:14:25.497044",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>chosen</th>\n",
       "      <th>QUESTION_CLEAN</th>\n",
       "      <th>ANSWER_CLEAN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7332</th>\n",
       "      <td>Human: John decides to start collecting art.  ...</td>\n",
       "      <td>The first 3 pieces each cost 45000/3=$15,000\\n...</td>\n",
       "      <td>human john decides to start collecting art  he...</td>\n",
       "      <td>the first pieces each cost so the next piece w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15658</th>\n",
       "      <td>Human: Identify the major theme of the followi...</td>\n",
       "      <td>The major theme of this excerpt is the pursuit...</td>\n",
       "      <td>human identify the major theme of the followin...</td>\n",
       "      <td>the major theme of this excerpt is the pursuit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11209</th>\n",
       "      <td>Human: Pretend you are a teacher explaining th...</td>\n",
       "      <td>The water cycle, also known as the hydrologic ...</td>\n",
       "      <td>human pretend you are a teacher explaining the...</td>\n",
       "      <td>the water cycle also known as the hydrologic c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1725</th>\n",
       "      <td>Human: Explain the plot of the movie Inception...</td>\n",
       "      <td>Inception follows a skilled thief named Dom Co...</td>\n",
       "      <td>human explain the plot of the movie inception ...</td>\n",
       "      <td>inception follows a skilled thief named dom co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6071</th>\n",
       "      <td>Human: Take the following text and rewrite it ...</td>\n",
       "      <td>Dear colleagues, I wanted to inform you that o...</td>\n",
       "      <td>human take the following text and rewrite it i...</td>\n",
       "      <td>dear colleagues i wanted to inform you that ou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6310</th>\n",
       "      <td>Human: Tracy, John and Jake found their combin...</td>\n",
       "      <td>Jake weighs 8kg more than 52 kg which is 8+52 ...</td>\n",
       "      <td>human tracy john and jake found their combined...</td>\n",
       "      <td>jake weighs more than kg which is kg tracy and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17757</th>\n",
       "      <td>Human: Rewrite the following paragraph in a mo...</td>\n",
       "      <td>Greetings. Upon reflecting on the influence of...</td>\n",
       "      <td>human rewrite the following paragraph in a mor...</td>\n",
       "      <td>greetings upon reflecting on the influence of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9640</th>\n",
       "      <td>Human: Read the paragraph and find any spellin...</td>\n",
       "      <td>Jake had just gotten his new puppy when sudden...</td>\n",
       "      <td>human read the paragraph and find any spelling...</td>\n",
       "      <td>jake had just gotten his new puppy when sudden...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14736</th>\n",
       "      <td>Human: Imagine you're a detective trying to so...</td>\n",
       "      <td>1. Collect and analyze the surveillance footag...</td>\n",
       "      <td>human imagine you are a detective trying to so...</td>\n",
       "      <td>collect and analyze the surveillance footage t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>788</th>\n",
       "      <td>Human: Translate the given haiku into English ...</td>\n",
       "      <td>Translation: An old pond, a frog jumps in; the...</td>\n",
       "      <td>human translate the given haiku into english a...</td>\n",
       "      <td>translation an old pond a frog jumps in the so...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  prompt  \\\n",
       "7332   Human: John decides to start collecting art.  ...   \n",
       "15658  Human: Identify the major theme of the followi...   \n",
       "11209  Human: Pretend you are a teacher explaining th...   \n",
       "1725   Human: Explain the plot of the movie Inception...   \n",
       "6071   Human: Take the following text and rewrite it ...   \n",
       "...                                                  ...   \n",
       "6310   Human: Tracy, John and Jake found their combin...   \n",
       "17757  Human: Rewrite the following paragraph in a mo...   \n",
       "9640   Human: Read the paragraph and find any spellin...   \n",
       "14736  Human: Imagine you're a detective trying to so...   \n",
       "788    Human: Translate the given haiku into English ...   \n",
       "\n",
       "                                                  chosen  \\\n",
       "7332   The first 3 pieces each cost 45000/3=$15,000\\n...   \n",
       "15658  The major theme of this excerpt is the pursuit...   \n",
       "11209  The water cycle, also known as the hydrologic ...   \n",
       "1725   Inception follows a skilled thief named Dom Co...   \n",
       "6071   Dear colleagues, I wanted to inform you that o...   \n",
       "...                                                  ...   \n",
       "6310   Jake weighs 8kg more than 52 kg which is 8+52 ...   \n",
       "17757  Greetings. Upon reflecting on the influence of...   \n",
       "9640   Jake had just gotten his new puppy when sudden...   \n",
       "14736  1. Collect and analyze the surveillance footag...   \n",
       "788    Translation: An old pond, a frog jumps in; the...   \n",
       "\n",
       "                                          QUESTION_CLEAN  \\\n",
       "7332   human john decides to start collecting art  he...   \n",
       "15658  human identify the major theme of the followin...   \n",
       "11209  human pretend you are a teacher explaining the...   \n",
       "1725   human explain the plot of the movie inception ...   \n",
       "6071   human take the following text and rewrite it i...   \n",
       "...                                                  ...   \n",
       "6310   human tracy john and jake found their combined...   \n",
       "17757  human rewrite the following paragraph in a mor...   \n",
       "9640   human read the paragraph and find any spelling...   \n",
       "14736  human imagine you are a detective trying to so...   \n",
       "788    human translate the given haiku into english a...   \n",
       "\n",
       "                                            ANSWER_CLEAN  \n",
       "7332   the first pieces each cost so the next piece w...  \n",
       "15658  the major theme of this excerpt is the pursuit...  \n",
       "11209  the water cycle also known as the hydrologic c...  \n",
       "1725   inception follows a skilled thief named dom co...  \n",
       "6071   dear colleagues i wanted to inform you that ou...  \n",
       "...                                                  ...  \n",
       "6310   jake weighs more than kg which is kg tracy and...  \n",
       "17757  greetings upon reflecting on the influence of ...  \n",
       "9640   jake had just gotten his new puppy when sudden...  \n",
       "14736  collect and analyze the surveillance footage t...  \n",
       "788    translation an old pond a frog jumps in the so...  \n",
       "\n",
       "[23100 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_parquet(\"/kaggle/input/gsm-general-qa/gsm_general_qa_instructions_train.parquet\")\n",
    "df.dropna(axis=0, how='any', inplace=True)\n",
    "df['QUESTION_CLEAN'] = df['prompt'].apply(lambda x: preprocess_sentence(x))\n",
    "df['ANSWER_CLEAN'] = df['chosen'].apply(lambda x: preprocess_sentence(x))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c51fcc23",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-03T05:14:35.457695Z",
     "iopub.status.busy": "2024-02-03T05:14:35.457395Z",
     "iopub.status.idle": "2024-02-03T05:14:37.549406Z",
     "shell.execute_reply": "2024-02-03T05:14:37.548576Z"
    },
    "papermill": {
     "duration": 2.102922,
     "end_time": "2024-02-03T05:14:37.551867",
     "exception": false,
     "start_time": "2024-02-03T05:14:35.448945",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"/kaggle/input/ubuntu-datasets/ubuntu_qa.parquet\")\n",
    "df.head()\n",
    "df.dropna(axis=0, how='any', inplace=True)\n",
    "df['QUESTION_CLEAN'] = df['question'].apply(lambda x: preprocess_sentence(x))\n",
    "df['ANSWER_CLEAN'] = df['answer'].apply(lambda x: preprocess_sentence(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83735304",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-03T05:14:37.569168Z",
     "iopub.status.busy": "2024-02-03T05:14:37.568845Z",
     "iopub.status.idle": "2024-02-03T05:14:37.614838Z",
     "shell.execute_reply": "2024-02-03T05:14:37.613751Z"
    },
    "papermill": {
     "duration": 0.05721,
     "end_time": "2024-02-03T05:14:37.617108",
     "exception": false,
     "start_time": "2024-02-03T05:14:37.559898",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.drop(df[df.ANSWER_CLEAN ==\"\"].index, inplace=True)\n",
    "df.drop(df[df.ANSWER_CLEAN.str.split().str.len() <16].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f894ce92",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-03T05:14:37.633751Z",
     "iopub.status.busy": "2024-02-03T05:14:37.633399Z",
     "iopub.status.idle": "2024-02-03T05:14:37.646047Z",
     "shell.execute_reply": "2024-02-03T05:14:37.645118Z"
    },
    "papermill": {
     "duration": 0.023141,
     "end_time": "2024-02-03T05:14:37.648040",
     "exception": false,
     "start_time": "2024-02-03T05:14:37.624899",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>QUESTION_CLEAN</th>\n",
       "      <th>ANSWER_CLEAN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is there a difference between using 'bash scri...</td>\n",
       "      <td>./foo.sh executes the file. if the file starts...</td>\n",
       "      <td>is there a difference between using bash scrip...</td>\n",
       "      <td>foosh executes the file if the file starts wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>any help with figuring out what driver my x se...</td>\n",
       "      <td>the x server isn't just the video driver, its ...</td>\n",
       "      <td>any help with figuring out what driver my x se...</td>\n",
       "      <td>the x server is not just the video driver its ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>can anyone help me please installing ubuntu fr...</td>\n",
       "      <td>what is so hard in that? just download unetboo...</td>\n",
       "      <td>can anyone help me please installing ubuntu fr...</td>\n",
       "      <td>what is so hard in that just download unetboot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>it seems that 10.10 is still weak on drivers f...</td>\n",
       "      <td>how interesting... i wish i knew that before i...</td>\n",
       "      <td>it seems that is still weak on drivers for old...</td>\n",
       "      <td>how interesting i wish i knew that before i in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>hey all, i just created a new fat32 partition....</td>\n",
       "      <td>if this is on your hard drive and not flash ju...</td>\n",
       "      <td>hey all i just created a new partition however...</td>\n",
       "      <td>if this is on your hard drive and not flash ju...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12056</th>\n",
       "      <td>hi, how can i stop someone from resetting my r...</td>\n",
       "      <td>you may be able to disable usb &amp; cd booting in...</td>\n",
       "      <td>hi how can i stop someone from resetting my ro...</td>\n",
       "      <td>you may be able to disable usb  cd booting in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12063</th>\n",
       "      <td>when i install kde, do i get the option to cho...</td>\n",
       "      <td>yes... via gdm or kdm ... you can edit /etc/x1...</td>\n",
       "      <td>when i install kde do i get the option to choo...</td>\n",
       "      <td>yes via gdm or kdm  you can edit to configure ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12093</th>\n",
       "      <td>i have just got one game and it's looks like b...</td>\n",
       "      <td>it's probably not executable. either chmod it ...</td>\n",
       "      <td>i have just got one game and it is looks like ...</td>\n",
       "      <td>it is probably not executable either chmod it ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12096</th>\n",
       "      <td>hi, i need some help. i installed ubuntu 7.04 ...</td>\n",
       "      <td>edit the kernel boot options... nosplash quiet...</td>\n",
       "      <td>hi i need some help i installed ubuntu in my c...</td>\n",
       "      <td>edit the kernel boot options nosplash quiet  t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12099</th>\n",
       "      <td>question: how can i change my dns servers for ...</td>\n",
       "      <td>right click nm-applet in the gnome-panel then ...</td>\n",
       "      <td>question how can i change my dns servers for a...</td>\n",
       "      <td>right click nmapplet in the gnomepanel then ed...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1800 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                question  \\\n",
       "1      is there a difference between using 'bash scri...   \n",
       "4      any help with figuring out what driver my x se...   \n",
       "5      can anyone help me please installing ubuntu fr...   \n",
       "8      it seems that 10.10 is still weak on drivers f...   \n",
       "12     hey all, i just created a new fat32 partition....   \n",
       "...                                                  ...   \n",
       "12056  hi, how can i stop someone from resetting my r...   \n",
       "12063  when i install kde, do i get the option to cho...   \n",
       "12093  i have just got one game and it's looks like b...   \n",
       "12096  hi, i need some help. i installed ubuntu 7.04 ...   \n",
       "12099  question: how can i change my dns servers for ...   \n",
       "\n",
       "                                                  answer  \\\n",
       "1      ./foo.sh executes the file. if the file starts...   \n",
       "4      the x server isn't just the video driver, its ...   \n",
       "5      what is so hard in that? just download unetboo...   \n",
       "8      how interesting... i wish i knew that before i...   \n",
       "12     if this is on your hard drive and not flash ju...   \n",
       "...                                                  ...   \n",
       "12056  you may be able to disable usb & cd booting in...   \n",
       "12063  yes... via gdm or kdm ... you can edit /etc/x1...   \n",
       "12093  it's probably not executable. either chmod it ...   \n",
       "12096  edit the kernel boot options... nosplash quiet...   \n",
       "12099  right click nm-applet in the gnome-panel then ...   \n",
       "\n",
       "                                          QUESTION_CLEAN  \\\n",
       "1      is there a difference between using bash scrip...   \n",
       "4      any help with figuring out what driver my x se...   \n",
       "5      can anyone help me please installing ubuntu fr...   \n",
       "8      it seems that is still weak on drivers for old...   \n",
       "12     hey all i just created a new partition however...   \n",
       "...                                                  ...   \n",
       "12056  hi how can i stop someone from resetting my ro...   \n",
       "12063  when i install kde do i get the option to choo...   \n",
       "12093  i have just got one game and it is looks like ...   \n",
       "12096  hi i need some help i installed ubuntu in my c...   \n",
       "12099  question how can i change my dns servers for a...   \n",
       "\n",
       "                                            ANSWER_CLEAN  \n",
       "1      foosh executes the file if the file starts wit...  \n",
       "4      the x server is not just the video driver its ...  \n",
       "5      what is so hard in that just download unetboot...  \n",
       "8      how interesting i wish i knew that before i in...  \n",
       "12     if this is on your hard drive and not flash ju...  \n",
       "...                                                  ...  \n",
       "12056  you may be able to disable usb  cd booting in ...  \n",
       "12063  yes via gdm or kdm  you can edit to configure ...  \n",
       "12093  it is probably not executable either chmod it ...  \n",
       "12096  edit the kernel boot options nosplash quiet  t...  \n",
       "12099  right click nmapplet in the gnomepanel then ed...  \n",
       "\n",
       "[1800 rows x 4 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a942cc0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-03T05:14:37.665629Z",
     "iopub.status.busy": "2024-02-03T05:14:37.665055Z",
     "iopub.status.idle": "2024-02-03T05:14:37.964130Z",
     "shell.execute_reply": "2024-02-03T05:14:37.963210Z"
    },
    "papermill": {
     "duration": 0.310414,
     "end_time": "2024-02-03T05:14:37.966269",
     "exception": false,
     "start_time": "2024-02-03T05:14:37.655855",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOCAB_SIZE: 6694\n",
      "INPUT_SEQ_LEN: 19\n",
      "TARGET_SEQ_LEN: 20\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "special_tokens = ['<SOS', '<EOS>', '<UNK>', '<PAD>']\n",
    "\n",
    "# create zip from querstion and answer\n",
    "qa_pairs = list(zip(df['QUESTION_CLEAN'], df['ANSWER_CLEAN']))\n",
    "\n",
    "\n",
    "# Split into training and validation sets\n",
    "train_pairs, val_pairs = train_test_split(qa_pairs, test_size=0.5, random_state=42)\n",
    "\n",
    "# Separate questions and answers for convenience\n",
    "train_questions, train_answers = zip(*train_pairs)\n",
    "val_questions, val_answers = zip(*val_pairs)\n",
    "\n",
    "# Define the tokenizer\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "# Function to yield list of tokens\n",
    "def yield_tokens(data_iter):\n",
    "    for text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "# Build vocabulary\n",
    "train_texts = train_questions + train_answers + val_questions + val_answers\n",
    "VOCAB = build_vocab_from_iterator(yield_tokens(train_texts), specials=['<PAD>', '<SOS>', '<EOS>', '<UNK>'])\n",
    "VOCAB.set_default_index(VOCAB['<UNK>'])\n",
    "\n",
    "VOCAB_SIZE = len(VOCAB)\n",
    "# INPUT_SEQ_LEN = 25\n",
    "# TARGET_SEQ_LEN = 25\n",
    "\n",
    "# INPUT_SEQ_LEN = df['QUESTION_CLEAN'].str.split().str.len().max()\n",
    "# TARGET_SEQ_LEN = df['ANSWER_CLEAN'].str.split().str.len().max()\n",
    "\n",
    "INPUT_SEQ_LEN = df['QUESTION_CLEAN'].str.split().str.len().median().__int__()\n",
    "TARGET_SEQ_LEN = df['ANSWER_CLEAN'].str.split().str.len().median().__int__()\n",
    "\n",
    "\n",
    "print('VOCAB_SIZE:', VOCAB_SIZE)\n",
    "print('INPUT_SEQ_LEN:', INPUT_SEQ_LEN)\n",
    "print('TARGET_SEQ_LEN:', TARGET_SEQ_LEN)\n",
    "\n",
    "\n",
    "def tokens_to_text(tokens):\n",
    "    # check if token is tensor or numpy array\n",
    "    if isinstance(tokens, torch.Tensor):\n",
    "        tokens = tokens.cpu().numpy()\n",
    "    special_tokens = np.array([VOCAB['<SOS>'], VOCAB['<PAD>'], VOCAB['<UNK>'], VOCAB['<EOS>']])\n",
    "    tokens = [token for token in tokens if token not in special_tokens]\n",
    "    return ' '.join(VOCAB.lookup_tokens(tokens))\n",
    "\n",
    "\n",
    "def text_to_tokens(text):\n",
    "    return [VOCAB[token] for token in tokenizer(text)]\n",
    "\n",
    "# Create a custom PyTorch Dataset\n",
    "class QADataset(Dataset):\n",
    "    def __init__(self, df, vocab, tokenizer, input_seq_len, target_seq_len):\n",
    "        self.df = df\n",
    "        self.vocab = vocab\n",
    "        self.tokenizer = tokenizer\n",
    "        self.input_seq_len = input_seq_len\n",
    "        self.target_seq_len = target_seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        question, answer = row['QUESTION_CLEAN'], row['ANSWER_CLEAN']\n",
    "\n",
    "        # Tokenize and encode the sequences\n",
    "        question_tokens = text_to_tokens(question)\n",
    "        answer_tokens = text_to_tokens(answer)\n",
    "\n",
    "        # Pad the sequences\n",
    "        enc_src = self.pad_sequence(question_tokens + [self.vocab['<EOS>']], self.input_seq_len)\n",
    "        dec_src = self.pad_sequence([self.vocab['<SOS>']] + answer_tokens, self.target_seq_len)\n",
    "        trg = self.pad_sequence([self.vocab['<SOS>']] + answer_tokens + [self.vocab['<EOS>']], self.target_seq_len)\n",
    "\n",
    "        return enc_src, dec_src, trg\n",
    "\n",
    "    def pad_sequence(self, seq, max_len):\n",
    "        return F.pad(torch.LongTensor(seq), (0, max_len - len(seq)), mode='constant', value=self.vocab['<PAD>'])\n",
    "    \n",
    "\n",
    "dataset = QADataset(df, VOCAB, tokenizer, INPUT_SEQ_LEN, TARGET_SEQ_LEN)\n",
    "\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "out = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ea28a09",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-03T05:14:37.983057Z",
     "iopub.status.busy": "2024-02-03T05:14:37.982779Z",
     "iopub.status.idle": "2024-02-03T05:14:38.020967Z",
     "shell.execute_reply": "2024-02-03T05:14:38.020027Z"
    },
    "papermill": {
     "duration": 0.049023,
     "end_time": "2024-02-03T05:14:38.022972",
     "exception": false,
     "start_time": "2024-02-03T05:14:37.973949",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, emb_size, heads):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.emb_size = emb_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = emb_size // heads\n",
    "        \n",
    "        assert (self.head_dim * heads == emb_size), \"Embedding size needs to be divisible by heads\"\n",
    "\n",
    "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.fc_out = nn.Linear(heads * self.head_dim, emb_size)\n",
    "\n",
    "    def forward(self, values, keys, query, mask):\n",
    "        batch_size = query.shape[0]\n",
    "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
    "        \n",
    "        # Split the embedding into self.heads different pieces\n",
    "        values = values.reshape(batch_size, value_len, self.heads, self.head_dim)\n",
    "        keys = keys.reshape(batch_size, key_len, self.heads, self.head_dim)\n",
    "        queries = query.reshape(batch_size, query_len, self.heads, self.head_dim)\n",
    "\n",
    "        values = self.values(values)\n",
    "        keys = self.keys(keys)\n",
    "        queries = self.queries(queries)\n",
    "\n",
    "        # Calculate energy\n",
    "        # energy = torch.matmul(queries, keys.transpose(1, 2)) # (batch_size, head, query_len, key_len)')\n",
    "        energy = torch.einsum('bqhd,bkhd->bhqk', [queries, keys]) # (batch_size, head, query_len, key_len)')\n",
    "\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "\n",
    "        d_k = keys.shape[3]\n",
    "        attention = torch.softmax(energy / (d_k ** 0.5), dim=3) # (batch_size, head, query_len, key_len)\n",
    "\n",
    "        out = torch.einsum('bhqk,bvhd->bqhd', [attention, values]) # (batch_size, query_len, head, embed_dim)\n",
    "        out = out.reshape(batch_size, query_len, self.heads * self.head_dim) # (batch_size, query_len, embed_dim)\n",
    "        out = self.fc_out(out) # (batch_size, query_len, embed_dim)\n",
    "        return out\n",
    "     \n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, emb_size, heads, forward_expansion, drop_out) -> None:\n",
    "        super(TransformerBlock, self).__init__()\n",
    "\n",
    "        self.attention = SelfAttention(emb_size, heads)\n",
    "        self.norm1 = nn.LayerNorm(emb_size)\n",
    "        self.norm2 = nn.LayerNorm(emb_size)\n",
    "\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(in_features=emb_size, out_features=forward_expansion * emb_size, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=forward_expansion * emb_size, out_features=emb_size, bias=True),\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(drop_out)\n",
    "\n",
    "    def forward(self, value, key, query, mask):\n",
    "        attention = self.attention(value, key, query, mask)\n",
    "\n",
    "        # Add skip connection, run through normalization and finally dropout\n",
    "        norm = self.norm1(attention + query)\n",
    "        norm = self.dropout(norm)\n",
    "\n",
    "        forward = self.feed_forward(norm)\n",
    "        out = self.norm2(forward + norm)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "class WordPositionEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, max_seq_len, emb_size, device, fixed=True):\n",
    "        super(WordPositionEmbedding, self).__init__()\n",
    "        \n",
    "        self.device = device\n",
    "        self.fixed = fixed\n",
    "\n",
    "        self.word_embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=emb_size, device=device)\n",
    "        \n",
    "        if fixed:\n",
    "            # Create fixed (non-learnable) position embeddings\n",
    "            position = torch.arange(max_seq_len).unsqueeze(1)\n",
    "            div_term = torch.exp(torch.arange(0, emb_size, 2).float() * (-math.log(10000.0) / emb_size))\n",
    "            position_embedding = torch.zeros(max_seq_len, emb_size)\n",
    "            position_embedding[:, 0::2] = torch.sin(position * div_term)\n",
    "            position_embedding[:, 1::2] = torch.cos(position * div_term)\n",
    "            # Register position_embedding as a buffer\n",
    "            self.register_buffer('position_embedding', position_embedding)\n",
    "        else:\n",
    "            self.position_embedding = nn.Embedding(max_seq_len, emb_size)\n",
    "\n",
    "    def forward(self, X):\n",
    "        batch_size, seq_len = X.shape\n",
    "\n",
    "        # Get word embeddings\n",
    "        word = self.word_embedding(X)\n",
    "\n",
    "        if self.fixed:\n",
    "            # Use fixed position embeddings\n",
    "            position = self.position_embedding[:seq_len, :]\n",
    "        else:\n",
    "            # Get position embeddings\n",
    "            position_ids = torch.arange(seq_len, device=self.device).unsqueeze(0).expand(batch_size, seq_len)\n",
    "            position = self.position_embedding(position_ids)\n",
    "        \n",
    "        # Add word and position embeddings\n",
    "        embeddings = word + position\n",
    "        return embeddings\n",
    "    \n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, seq_len, emb_size, n_layers, heads, forward_expansion, drop_out, device):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.emb_size = emb_size\n",
    "        self.device = device\n",
    "        \n",
    "        self.embedding = WordPositionEmbedding(vocab_size, seq_len, emb_size, device, fixed=True)\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                TransformerBlock(emb_size, heads, forward_expansion, drop_out) for _ in range(n_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(drop_out)\n",
    "\n",
    "    def forward(self, X, mask):\n",
    "        batch_size, seq_len = X.shape\n",
    "        out = self.dropout(self.embedding(X))\n",
    "\n",
    "        for layer in self.layers:\n",
    "            out = layer(out, out, out, mask)\n",
    "        return out\n",
    "        \n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, emb_size, heads, forward_expansion, drop_out) -> None:\n",
    "        super(DecoderBlock, self).__init__()\n",
    "\n",
    "        self.attention = SelfAttention(emb_size, heads)\n",
    "        self.norm = nn.LayerNorm(emb_size)\n",
    "        self.transformer_block = TransformerBlock(emb_size, heads, forward_expansion, drop_out)\n",
    "        self.dropout = nn.Dropout(drop_out)\n",
    "    \n",
    "    def forward(self, X, value, key, src_mask, trg_mask):\n",
    "        attention = self.attention(X, X, X, trg_mask)\n",
    "        query = self.dropout(self.norm(attention + X))\n",
    "        out = self.transformer_block(value, key, query, src_mask)\n",
    "        return out\n",
    "    \n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, seq_len, emb_size, n_layers, heads, forward_expansion, drop_out, device) -> None:\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.device = device\n",
    "        self.embedding = WordPositionEmbedding(vocab_size, seq_len, emb_size, device, fixed=True)\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                DecoderBlock(emb_size, heads, forward_expansion, drop_out) for _ in range(n_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.fc_out = nn.Linear(emb_size, vocab_size)\n",
    "        self.dropout = nn.Dropout(drop_out)\n",
    "\n",
    "    def forward(self, X, enc_out, src_mask, trg_mask):\n",
    "        batch_size, seq_len = X.shape\n",
    "\n",
    "        out = self.dropout(self.embedding(X))\n",
    "\n",
    "        for layer in self.layers:\n",
    "            out = layer(out, enc_out, enc_out, src_mask, trg_mask)\n",
    "\n",
    "        out = self.fc_out(out)\n",
    "        return out\n",
    "    \n",
    "\n",
    "class TransformerScratch(nn.Module):\n",
    "    def __init__(self, \n",
    "                 inp_vocab_size, \n",
    "                 trg_vocab_size,\n",
    "                 src_pad_idx,\n",
    "                 trg_pad_idx,\n",
    "                 emb_size,\n",
    "                 n_layers=1,\n",
    "                 heads=1,\n",
    "                 forward_expansion=1,\n",
    "                 drop_out=0.2,\n",
    "                 max_seq_len=100,\n",
    "                 device=torch.device('cuda'),\n",
    "                 ) -> None:\n",
    "        super(TransformerScratch, self).__init__()\n",
    "\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.device = device\n",
    "\n",
    "        self.encoder = Encoder(inp_vocab_size, max_seq_len, emb_size, n_layers, heads, forward_expansion, drop_out, device).to(device)\n",
    "        self.decoder = Decoder(trg_vocab_size, max_seq_len, emb_size, n_layers, heads, forward_expansion, drop_out, device).to(device)\n",
    "\n",
    "    def make_src_mask(self, src):\n",
    "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        # (batch_size, 1, 1, src_len)\n",
    "        return src_mask.to(self.device)\n",
    "    \n",
    "    def make_trg_mask(self, trg):\n",
    "        batch_size, trg_seq_len = trg.shape\n",
    "        trg_mask = torch.tril(torch.ones((trg_seq_len, trg_seq_len))).expand(batch_size, 1, trg_seq_len, trg_seq_len)\n",
    "        # (batch_size, 1, trg_len, trg_len)\n",
    "        return trg_mask.to(self.device)\n",
    "\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        src_mask = self.make_src_mask(src)\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "        enc_out = self.encoder(src, src_mask)\n",
    "        out = self.decoder(trg, enc_out, src_mask, trg_mask)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b425ed9b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-03T05:14:38.039608Z",
     "iopub.status.busy": "2024-02-03T05:14:38.039126Z",
     "iopub.status.idle": "2024-02-03T05:14:38.053851Z",
     "shell.execute_reply": "2024-02-03T05:14:38.053016Z"
    },
    "papermill": {
     "duration": 0.02497,
     "end_time": "2024-02-03T05:14:38.055700",
     "exception": false,
     "start_time": "2024-02-03T05:14:38.030730",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def step(model, enc_src, dec_src, trg, loss_fn, VOCAB, device):\n",
    "    enc_src = enc_src.to(device)\n",
    "    dec_src = dec_src.to(device)\n",
    "    trg = trg.to(device)\n",
    "\n",
    "    # Forward pass through the model\n",
    "    logits = model(enc_src, dec_src)\n",
    "\n",
    "    # Shift so we don't include the SOS token in targets, and remove the last logit to match targets\n",
    "    logits = logits[:, :-1, :].contiguous()\n",
    "    trg = trg[:, 1:].contiguous()\n",
    "    \n",
    "    loss = loss_fn(logits.view(-1, logits.shape[-1]), trg.view(-1))\n",
    "    # loss = loss_fn(logits.permute(0, 2, 1), trg)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    non_pad_elements = (trg != VOCAB['<PAD>']).nonzero(as_tuple=True)\n",
    "    correct_predictions = (logits.argmax(dim=2) == trg).sum().item()\n",
    "    accuracy = correct_predictions / len(non_pad_elements[0])\n",
    "\n",
    "    return loss, accuracy\n",
    "\n",
    "def train_step(model, iterator, optimizer, loss_fn, clip, VOCAB, device):\n",
    "    model.train()  # Set the model to training mode\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    for i, batch in enumerate(iterator):\n",
    "        enc_src, dec_src, trg = batch\n",
    "        \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss, accuracy = step(model, enc_src, dec_src, trg, loss_fn, VOCAB, device)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip gradients to prevent exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate loss and accuracy\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += accuracy\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "\n",
    "def evaluate_step(model, iterator, loss_fn, VOCAB, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for i, batch in enumerate(iterator):\n",
    "            enc_src, dec_src, trg = batch\n",
    "\n",
    "            loss, accuracy = step(model, enc_src, dec_src, trg, loss_fn, VOCAB, device)\n",
    "\n",
    "            # Accumulate loss and accuracy\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += accuracy\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "\n",
    "def train(model, train_loader, optimizer, loss_fn, clip, epochs, VOCAB, device, val_loader=None):\n",
    "    for epoch in range(epochs):\n",
    "        train_loss, train_acc = train_step(model, train_loader, optimizer, loss_fn, clip, VOCAB, device)\n",
    "        result = f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc * 100:.2f}%'\n",
    "\n",
    "        if val_loader:\n",
    "            eval_loss, eval_acc = evaluate_step(model, val_loader, loss_fn, VOCAB, device)\n",
    "            result += f'|| Eval Loss: {eval_loss:.3f} | Eval Acc: {eval_acc * 100:.2f}%'\n",
    "            \n",
    "        print(f'Epoch: {epoch + 1:02}')\n",
    "        print(result)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25400e84",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-03T05:14:38.071888Z",
     "iopub.status.busy": "2024-02-03T05:14:38.071602Z",
     "iopub.status.idle": "2024-02-03T05:16:19.823723Z",
     "shell.execute_reply": "2024-02-03T05:16:19.822740Z"
    },
    "papermill": {
     "duration": 101.762658,
     "end_time": "2024-02-03T05:16:19.825755",
     "exception": false,
     "start_time": "2024-02-03T05:14:38.063097",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01\n",
      "\tTrain Loss: 6.817 | Train Acc: 3.90%\n",
      "Epoch: 02\n",
      "\tTrain Loss: 6.245 | Train Acc: 4.08%\n",
      "Epoch: 03\n",
      "\tTrain Loss: 6.219 | Train Acc: 4.33%\n",
      "Epoch: 04\n",
      "\tTrain Loss: 6.204 | Train Acc: 4.30%\n",
      "Epoch: 05\n",
      "\tTrain Loss: 6.200 | Train Acc: 4.22%\n",
      "Epoch: 06\n",
      "\tTrain Loss: 6.198 | Train Acc: 3.91%\n",
      "Epoch: 07\n",
      "\tTrain Loss: 6.202 | Train Acc: 3.95%\n",
      "Epoch: 08\n",
      "\tTrain Loss: 6.199 | Train Acc: 3.99%\n",
      "Epoch: 09\n",
      "\tTrain Loss: 6.198 | Train Acc: 4.21%\n",
      "Epoch: 10\n",
      "\tTrain Loss: 6.186 | Train Acc: 4.30%\n",
      "Epoch: 11\n",
      "\tTrain Loss: 6.187 | Train Acc: 4.24%\n",
      "Epoch: 12\n",
      "\tTrain Loss: 6.180 | Train Acc: 4.24%\n",
      "Epoch: 13\n",
      "\tTrain Loss: 6.162 | Train Acc: 3.93%\n",
      "Epoch: 14\n",
      "\tTrain Loss: 6.150 | Train Acc: 4.21%\n",
      "Epoch: 15\n",
      "\tTrain Loss: 6.128 | Train Acc: 4.30%\n",
      "Epoch: 16\n",
      "\tTrain Loss: 6.114 | Train Acc: 4.31%\n",
      "Epoch: 17\n",
      "\tTrain Loss: 6.113 | Train Acc: 4.39%\n",
      "Epoch: 18\n",
      "\tTrain Loss: 6.085 | Train Acc: 4.35%\n",
      "Epoch: 19\n",
      "\tTrain Loss: 6.072 | Train Acc: 4.68%\n",
      "Epoch: 20\n",
      "\tTrain Loss: 5.980 | Train Acc: 7.03%\n",
      "Epoch: 21\n",
      "\tTrain Loss: 5.696 | Train Acc: 8.97%\n",
      "Epoch: 22\n",
      "\tTrain Loss: 5.351 | Train Acc: 9.29%\n",
      "Epoch: 23\n",
      "\tTrain Loss: 5.015 | Train Acc: 9.30%\n",
      "Epoch: 24\n",
      "\tTrain Loss: 4.715 | Train Acc: 9.11%\n",
      "Epoch: 25\n",
      "\tTrain Loss: 4.428 | Train Acc: 9.22%\n",
      "Epoch: 26\n",
      "\tTrain Loss: 4.170 | Train Acc: 9.19%\n",
      "Epoch: 27\n",
      "\tTrain Loss: 3.963 | Train Acc: 8.89%\n",
      "Epoch: 28\n",
      "\tTrain Loss: 3.801 | Train Acc: 8.92%\n",
      "Epoch: 29\n",
      "\tTrain Loss: 3.688 | Train Acc: 8.89%\n",
      "Epoch: 30\n",
      "\tTrain Loss: 3.596 | Train Acc: 8.64%\n",
      "Epoch: 31\n",
      "\tTrain Loss: 3.539 | Train Acc: 8.42%\n",
      "Epoch: 32\n",
      "\tTrain Loss: 3.472 | Train Acc: 8.53%\n",
      "Epoch: 33\n",
      "\tTrain Loss: 3.424 | Train Acc: 8.19%\n",
      "Epoch: 34\n",
      "\tTrain Loss: 3.382 | Train Acc: 8.22%\n",
      "Epoch: 35\n",
      "\tTrain Loss: 3.353 | Train Acc: 8.16%\n",
      "Epoch: 36\n",
      "\tTrain Loss: 3.342 | Train Acc: 8.16%\n",
      "Epoch: 37\n",
      "\tTrain Loss: 3.313 | Train Acc: 8.23%\n",
      "Epoch: 38\n",
      "\tTrain Loss: 3.307 | Train Acc: 7.95%\n",
      "Epoch: 39\n",
      "\tTrain Loss: 3.284 | Train Acc: 7.95%\n",
      "Epoch: 40\n",
      "\tTrain Loss: 3.284 | Train Acc: 7.81%\n",
      "Epoch: 41\n",
      "\tTrain Loss: 3.266 | Train Acc: 8.02%\n",
      "Epoch: 42\n",
      "\tTrain Loss: 3.257 | Train Acc: 7.86%\n",
      "Epoch: 43\n",
      "\tTrain Loss: 3.242 | Train Acc: 8.04%\n",
      "Epoch: 44\n",
      "\tTrain Loss: 3.222 | Train Acc: 7.86%\n",
      "Epoch: 45\n",
      "\tTrain Loss: 3.234 | Train Acc: 7.99%\n",
      "Epoch: 46\n",
      "\tTrain Loss: 3.219 | Train Acc: 7.95%\n",
      "Epoch: 47\n",
      "\tTrain Loss: 3.204 | Train Acc: 7.70%\n",
      "Epoch: 48\n",
      "\tTrain Loss: 3.203 | Train Acc: 8.10%\n",
      "Epoch: 49\n",
      "\tTrain Loss: 3.196 | Train Acc: 8.05%\n",
      "Epoch: 50\n",
      "\tTrain Loss: 3.188 | Train Acc: 7.90%\n",
      "Epoch: 51\n",
      "\tTrain Loss: 3.184 | Train Acc: 7.98%\n",
      "Epoch: 52\n",
      "\tTrain Loss: 3.183 | Train Acc: 7.84%\n",
      "Epoch: 53\n",
      "\tTrain Loss: 3.161 | Train Acc: 8.07%\n",
      "Epoch: 54\n",
      "\tTrain Loss: 3.172 | Train Acc: 7.78%\n",
      "Epoch: 55\n",
      "\tTrain Loss: 3.163 | Train Acc: 8.00%\n",
      "Epoch: 56\n",
      "\tTrain Loss: 3.165 | Train Acc: 7.99%\n",
      "Epoch: 57\n",
      "\tTrain Loss: 3.158 | Train Acc: 8.00%\n",
      "Epoch: 58\n",
      "\tTrain Loss: 3.158 | Train Acc: 7.89%\n",
      "Epoch: 59\n",
      "\tTrain Loss: 3.144 | Train Acc: 7.95%\n",
      "Epoch: 60\n",
      "\tTrain Loss: 3.152 | Train Acc: 7.82%\n",
      "Epoch: 61\n",
      "\tTrain Loss: 3.147 | Train Acc: 7.88%\n",
      "Epoch: 62\n",
      "\tTrain Loss: 3.134 | Train Acc: 8.06%\n",
      "Epoch: 63\n",
      "\tTrain Loss: 3.131 | Train Acc: 7.80%\n",
      "Epoch: 64\n",
      "\tTrain Loss: 3.124 | Train Acc: 8.22%\n",
      "Epoch: 65\n",
      "\tTrain Loss: 3.112 | Train Acc: 8.11%\n",
      "Epoch: 66\n",
      "\tTrain Loss: 3.119 | Train Acc: 7.97%\n",
      "Epoch: 67\n",
      "\tTrain Loss: 3.109 | Train Acc: 8.06%\n",
      "Epoch: 68\n",
      "\tTrain Loss: 3.113 | Train Acc: 7.98%\n",
      "Epoch: 69\n",
      "\tTrain Loss: 3.112 | Train Acc: 7.95%\n",
      "Epoch: 70\n",
      "\tTrain Loss: 3.116 | Train Acc: 7.89%\n",
      "Epoch: 71\n",
      "\tTrain Loss: 3.111 | Train Acc: 8.00%\n",
      "Epoch: 72\n",
      "\tTrain Loss: 3.108 | Train Acc: 7.90%\n",
      "Epoch: 73\n",
      "\tTrain Loss: 3.091 | Train Acc: 7.98%\n",
      "Epoch: 74\n",
      "\tTrain Loss: 3.094 | Train Acc: 8.24%\n",
      "Epoch: 75\n",
      "\tTrain Loss: 3.101 | Train Acc: 8.00%\n",
      "Epoch: 76\n",
      "\tTrain Loss: 3.094 | Train Acc: 7.96%\n",
      "Epoch: 77\n",
      "\tTrain Loss: 3.091 | Train Acc: 8.16%\n",
      "Epoch: 78\n",
      "\tTrain Loss: 3.090 | Train Acc: 8.20%\n",
      "Epoch: 79\n",
      "\tTrain Loss: 3.084 | Train Acc: 7.89%\n",
      "Epoch: 80\n",
      "\tTrain Loss: 3.079 | Train Acc: 8.05%\n",
      "Epoch: 81\n",
      "\tTrain Loss: 3.079 | Train Acc: 8.10%\n",
      "Epoch: 82\n",
      "\tTrain Loss: 3.077 | Train Acc: 7.88%\n",
      "Epoch: 83\n",
      "\tTrain Loss: 3.075 | Train Acc: 8.18%\n",
      "Epoch: 84\n",
      "\tTrain Loss: 3.079 | Train Acc: 8.00%\n",
      "Epoch: 85\n",
      "\tTrain Loss: 3.067 | Train Acc: 8.22%\n",
      "Epoch: 86\n",
      "\tTrain Loss: 3.062 | Train Acc: 8.02%\n",
      "Epoch: 87\n",
      "\tTrain Loss: 3.060 | Train Acc: 8.23%\n",
      "Epoch: 88\n",
      "\tTrain Loss: 3.063 | Train Acc: 8.25%\n",
      "Epoch: 89\n",
      "\tTrain Loss: 3.066 | Train Acc: 8.20%\n",
      "Epoch: 90\n",
      "\tTrain Loss: 3.058 | Train Acc: 8.01%\n",
      "Epoch: 91\n",
      "\tTrain Loss: 3.060 | Train Acc: 7.95%\n",
      "Epoch: 92\n",
      "\tTrain Loss: 3.052 | Train Acc: 8.07%\n",
      "Epoch: 93\n",
      "\tTrain Loss: 3.051 | Train Acc: 8.28%\n",
      "Epoch: 94\n",
      "\tTrain Loss: 3.046 | Train Acc: 8.23%\n",
      "Epoch: 95\n",
      "\tTrain Loss: 3.049 | Train Acc: 8.20%\n",
      "Epoch: 96\n",
      "\tTrain Loss: 3.046 | Train Acc: 8.09%\n",
      "Epoch: 97\n",
      "\tTrain Loss: 3.041 | Train Acc: 8.09%\n",
      "Epoch: 98\n",
      "\tTrain Loss: 3.036 | Train Acc: 8.09%\n",
      "Epoch: 99\n",
      "\tTrain Loss: 3.038 | Train Acc: 8.09%\n",
      "Epoch: 100\n",
      "\tTrain Loss: 3.038 | Train Acc: 8.19%\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "transformer = TransformerScratch(\n",
    "    inp_vocab_size = VOCAB_SIZE,\n",
    "    trg_vocab_size = VOCAB_SIZE,\n",
    "    src_pad_idx = VOCAB['<PAD>'],\n",
    "    trg_pad_idx = VOCAB['<PAD>'],\n",
    "    emb_size = 256,\n",
    "    n_layers=2,\n",
    "    heads=8,\n",
    "    forward_expansion=4,\n",
    "    drop_out=0.05,\n",
    "    max_seq_len=TARGET_SEQ_LEN,\n",
    "    device=device\n",
    ").to(device)\n",
    "\n",
    "loss_function = torch.nn.CrossEntropyLoss(ignore_index=VOCAB['<PAD>'], reduction='mean')\n",
    "optimizer = optim.Adam(transformer.parameters(), lr=0.001)\n",
    "\n",
    "transformer = train(transformer, dataloader, optimizer, loss_function, clip=1, epochs=100, VOCAB=VOCAB, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "83edd59d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-03T05:16:19.857545Z",
     "iopub.status.busy": "2024-02-03T05:16:19.857242Z",
     "iopub.status.idle": "2024-02-03T05:16:19.867923Z",
     "shell.execute_reply": "2024-02-03T05:16:19.866957Z"
    },
    "papermill": {
     "duration": 0.028866,
     "end_time": "2024-02-03T05:16:19.869927",
     "exception": false,
     "start_time": "2024-02-03T05:16:19.841061",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TransformerPytroch(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        inp_vocab_size, \n",
    "        trg_vocab_size,\n",
    "        src_pad_idx,\n",
    "        trg_pad_idx,\n",
    "        emb_size,\n",
    "        n_layers=1,\n",
    "        heads=1,\n",
    "        forward_expansion=1,\n",
    "        drop_out=0.2,\n",
    "        max_seq_len=100,\n",
    "        device=torch.device('cuda')\n",
    "    ):\n",
    "        super(TransformerPytroch, self).__init__()\n",
    "\n",
    "        self.enc_embedding = WordPositionEmbedding(inp_vocab_size, max_seq_len, emb_size, device, fixed=False)\n",
    "        self.dec_embedding = WordPositionEmbedding(trg_vocab_size, max_seq_len, emb_size, device, fixed=False)\n",
    "\n",
    "\n",
    "        self.device = device\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=emb_size,\n",
    "            nhead=heads,\n",
    "            num_encoder_layers=n_layers,\n",
    "            num_decoder_layers=n_layers,\n",
    "            dim_feedforward=forward_expansion,\n",
    "            dropout=drop_out,\n",
    "            batch_first=True,\n",
    "            device=device\n",
    "        )\n",
    "        self.fc_out = nn.Linear(emb_size, trg_vocab_size)\n",
    "        self.dropout = nn.Dropout(drop_out)\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "\n",
    "    def make_src_mask(self, src):\n",
    "        src_mask = src == self.src_pad_idx\n",
    "\n",
    "        # (N, src_len)\n",
    "        return src_mask.to(self.device)\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        batch_size, trg_seq_length = trg.shape\n",
    "\n",
    "        src_padding_mask = self.make_src_mask(src)\n",
    "        trg_mask = self.transformer.generate_square_subsequent_mask(trg_seq_length).to(\n",
    "            self.device\n",
    "        )\n",
    "\n",
    "        src_emb = self.dropout(self.enc_embedding(src))\n",
    "        trg_emb = self.dropout(self.dec_embedding(trg))\n",
    "\n",
    "        out = self.transformer(\n",
    "            src_emb,\n",
    "            trg_emb,\n",
    "            src_key_padding_mask=src_padding_mask,\n",
    "            tgt_mask=trg_mask,\n",
    "        )\n",
    "        out = self.fc_out(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "216d4a82",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-03T05:16:19.900870Z",
     "iopub.status.busy": "2024-02-03T05:16:19.900615Z",
     "iopub.status.idle": "2024-02-03T05:18:05.574930Z",
     "shell.execute_reply": "2024-02-03T05:18:05.573818Z"
    },
    "papermill": {
     "duration": 105.692244,
     "end_time": "2024-02-03T05:18:05.577170",
     "exception": false,
     "start_time": "2024-02-03T05:16:19.884926",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01\n",
      "\tTrain Loss: 6.675 | Train Acc: 4.55%\n",
      "Epoch: 02\n",
      "\tTrain Loss: 6.070 | Train Acc: 8.00%\n",
      "Epoch: 03\n",
      "\tTrain Loss: 5.739 | Train Acc: 10.06%\n",
      "Epoch: 04\n",
      "\tTrain Loss: 5.365 | Train Acc: 12.21%\n",
      "Epoch: 05\n",
      "\tTrain Loss: 4.941 | Train Acc: 14.58%\n",
      "Epoch: 06\n",
      "\tTrain Loss: 4.496 | Train Acc: 16.47%\n",
      "Epoch: 07\n",
      "\tTrain Loss: 4.024 | Train Acc: 19.72%\n",
      "Epoch: 08\n",
      "\tTrain Loss: 3.572 | Train Acc: 25.28%\n",
      "Epoch: 09\n",
      "\tTrain Loss: 3.146 | Train Acc: 31.80%\n",
      "Epoch: 10\n",
      "\tTrain Loss: 2.737 | Train Acc: 39.51%\n",
      "Epoch: 11\n",
      "\tTrain Loss: 2.385 | Train Acc: 46.75%\n",
      "Epoch: 12\n",
      "\tTrain Loss: 2.092 | Train Acc: 52.39%\n",
      "Epoch: 13\n",
      "\tTrain Loss: 1.826 | Train Acc: 57.38%\n",
      "Epoch: 14\n",
      "\tTrain Loss: 1.603 | Train Acc: 62.29%\n",
      "Epoch: 15\n",
      "\tTrain Loss: 1.415 | Train Acc: 66.42%\n",
      "Epoch: 16\n",
      "\tTrain Loss: 1.256 | Train Acc: 69.68%\n",
      "Epoch: 17\n",
      "\tTrain Loss: 1.130 | Train Acc: 72.20%\n",
      "Epoch: 18\n",
      "\tTrain Loss: 1.014 | Train Acc: 74.85%\n",
      "Epoch: 19\n",
      "\tTrain Loss: 0.915 | Train Acc: 77.11%\n",
      "Epoch: 20\n",
      "\tTrain Loss: 0.855 | Train Acc: 78.39%\n",
      "Epoch: 21\n",
      "\tTrain Loss: 0.791 | Train Acc: 79.74%\n",
      "Epoch: 22\n",
      "\tTrain Loss: 0.738 | Train Acc: 80.93%\n",
      "Epoch: 23\n",
      "\tTrain Loss: 0.677 | Train Acc: 82.46%\n",
      "Epoch: 24\n",
      "\tTrain Loss: 0.638 | Train Acc: 83.30%\n",
      "Epoch: 25\n",
      "\tTrain Loss: 0.605 | Train Acc: 84.04%\n",
      "Epoch: 26\n",
      "\tTrain Loss: 0.569 | Train Acc: 84.74%\n",
      "Epoch: 27\n",
      "\tTrain Loss: 0.555 | Train Acc: 84.91%\n",
      "Epoch: 28\n",
      "\tTrain Loss: 0.535 | Train Acc: 85.44%\n",
      "Epoch: 29\n",
      "\tTrain Loss: 0.506 | Train Acc: 86.45%\n",
      "Epoch: 30\n",
      "\tTrain Loss: 0.475 | Train Acc: 87.09%\n",
      "Epoch: 31\n",
      "\tTrain Loss: 0.463 | Train Acc: 87.51%\n",
      "Epoch: 32\n",
      "\tTrain Loss: 0.448 | Train Acc: 88.04%\n",
      "Epoch: 33\n",
      "\tTrain Loss: 0.433 | Train Acc: 88.03%\n",
      "Epoch: 34\n",
      "\tTrain Loss: 0.431 | Train Acc: 87.80%\n",
      "Epoch: 35\n",
      "\tTrain Loss: 0.404 | Train Acc: 88.97%\n",
      "Epoch: 36\n",
      "\tTrain Loss: 0.389 | Train Acc: 89.05%\n",
      "Epoch: 37\n",
      "\tTrain Loss: 0.390 | Train Acc: 89.11%\n",
      "Epoch: 38\n",
      "\tTrain Loss: 0.375 | Train Acc: 89.76%\n",
      "Epoch: 39\n",
      "\tTrain Loss: 0.359 | Train Acc: 90.00%\n",
      "Epoch: 40\n",
      "\tTrain Loss: 0.344 | Train Acc: 90.22%\n",
      "Epoch: 41\n",
      "\tTrain Loss: 0.340 | Train Acc: 90.44%\n",
      "Epoch: 42\n",
      "\tTrain Loss: 0.339 | Train Acc: 90.33%\n",
      "Epoch: 43\n",
      "\tTrain Loss: 0.321 | Train Acc: 90.79%\n",
      "Epoch: 44\n",
      "\tTrain Loss: 0.325 | Train Acc: 90.70%\n",
      "Epoch: 45\n",
      "\tTrain Loss: 0.307 | Train Acc: 91.32%\n",
      "Epoch: 46\n",
      "\tTrain Loss: 0.308 | Train Acc: 91.23%\n",
      "Epoch: 47\n",
      "\tTrain Loss: 0.300 | Train Acc: 91.59%\n",
      "Epoch: 48\n",
      "\tTrain Loss: 0.305 | Train Acc: 91.14%\n",
      "Epoch: 49\n",
      "\tTrain Loss: 0.296 | Train Acc: 91.35%\n",
      "Epoch: 50\n",
      "\tTrain Loss: 0.296 | Train Acc: 91.59%\n",
      "Epoch: 51\n",
      "\tTrain Loss: 0.281 | Train Acc: 91.88%\n",
      "Epoch: 52\n",
      "\tTrain Loss: 0.280 | Train Acc: 91.90%\n",
      "Epoch: 53\n",
      "\tTrain Loss: 0.276 | Train Acc: 91.93%\n",
      "Epoch: 54\n",
      "\tTrain Loss: 0.279 | Train Acc: 91.85%\n",
      "Epoch: 55\n",
      "\tTrain Loss: 0.268 | Train Acc: 92.32%\n",
      "Epoch: 56\n",
      "\tTrain Loss: 0.257 | Train Acc: 92.62%\n",
      "Epoch: 57\n",
      "\tTrain Loss: 0.255 | Train Acc: 92.84%\n",
      "Epoch: 58\n",
      "\tTrain Loss: 0.256 | Train Acc: 92.41%\n",
      "Epoch: 59\n",
      "\tTrain Loss: 0.246 | Train Acc: 92.91%\n",
      "Epoch: 60\n",
      "\tTrain Loss: 0.255 | Train Acc: 92.56%\n",
      "Epoch: 61\n",
      "\tTrain Loss: 0.243 | Train Acc: 92.97%\n",
      "Epoch: 62\n",
      "\tTrain Loss: 0.240 | Train Acc: 93.07%\n",
      "Epoch: 63\n",
      "\tTrain Loss: 0.238 | Train Acc: 93.11%\n",
      "Epoch: 64\n",
      "\tTrain Loss: 0.243 | Train Acc: 92.97%\n",
      "Epoch: 65\n",
      "\tTrain Loss: 0.233 | Train Acc: 93.10%\n",
      "Epoch: 66\n",
      "\tTrain Loss: 0.233 | Train Acc: 93.12%\n",
      "Epoch: 67\n",
      "\tTrain Loss: 0.230 | Train Acc: 93.08%\n",
      "Epoch: 68\n",
      "\tTrain Loss: 0.223 | Train Acc: 93.47%\n",
      "Epoch: 69\n",
      "\tTrain Loss: 0.223 | Train Acc: 93.42%\n",
      "Epoch: 70\n",
      "\tTrain Loss: 0.223 | Train Acc: 93.47%\n",
      "Epoch: 71\n",
      "\tTrain Loss: 0.211 | Train Acc: 93.77%\n",
      "Epoch: 72\n",
      "\tTrain Loss: 0.216 | Train Acc: 93.63%\n",
      "Epoch: 73\n",
      "\tTrain Loss: 0.210 | Train Acc: 93.80%\n",
      "Epoch: 74\n",
      "\tTrain Loss: 0.213 | Train Acc: 93.75%\n",
      "Epoch: 75\n",
      "\tTrain Loss: 0.216 | Train Acc: 93.59%\n",
      "Epoch: 76\n",
      "\tTrain Loss: 0.210 | Train Acc: 93.73%\n",
      "Epoch: 77\n",
      "\tTrain Loss: 0.217 | Train Acc: 93.62%\n",
      "Epoch: 78\n",
      "\tTrain Loss: 0.211 | Train Acc: 93.71%\n",
      "Epoch: 79\n",
      "\tTrain Loss: 0.210 | Train Acc: 93.96%\n",
      "Epoch: 80\n",
      "\tTrain Loss: 0.196 | Train Acc: 94.08%\n",
      "Epoch: 81\n",
      "\tTrain Loss: 0.196 | Train Acc: 94.21%\n",
      "Epoch: 82\n",
      "\tTrain Loss: 0.196 | Train Acc: 94.07%\n",
      "Epoch: 83\n",
      "\tTrain Loss: 0.193 | Train Acc: 94.31%\n",
      "Epoch: 84\n",
      "\tTrain Loss: 0.194 | Train Acc: 94.28%\n",
      "Epoch: 85\n",
      "\tTrain Loss: 0.203 | Train Acc: 94.06%\n",
      "Epoch: 86\n",
      "\tTrain Loss: 0.187 | Train Acc: 94.26%\n",
      "Epoch: 87\n",
      "\tTrain Loss: 0.185 | Train Acc: 94.49%\n",
      "Epoch: 88\n",
      "\tTrain Loss: 0.195 | Train Acc: 94.27%\n",
      "Epoch: 89\n",
      "\tTrain Loss: 0.189 | Train Acc: 94.45%\n",
      "Epoch: 90\n",
      "\tTrain Loss: 0.185 | Train Acc: 94.33%\n",
      "Epoch: 91\n",
      "\tTrain Loss: 0.180 | Train Acc: 94.53%\n",
      "Epoch: 92\n",
      "\tTrain Loss: 0.180 | Train Acc: 94.55%\n",
      "Epoch: 93\n",
      "\tTrain Loss: 0.176 | Train Acc: 94.76%\n",
      "Epoch: 94\n",
      "\tTrain Loss: 0.177 | Train Acc: 94.52%\n",
      "Epoch: 95\n",
      "\tTrain Loss: 0.176 | Train Acc: 94.71%\n",
      "Epoch: 96\n",
      "\tTrain Loss: 0.178 | Train Acc: 94.64%\n",
      "Epoch: 97\n",
      "\tTrain Loss: 0.178 | Train Acc: 94.58%\n",
      "Epoch: 98\n",
      "\tTrain Loss: 0.182 | Train Acc: 94.51%\n",
      "Epoch: 99\n",
      "\tTrain Loss: 0.172 | Train Acc: 94.84%\n",
      "Epoch: 100\n",
      "\tTrain Loss: 0.170 | Train Acc: 94.76%\n"
     ]
    }
   ],
   "source": [
    "pytorch_transformer = TransformerPytroch(\n",
    "    inp_vocab_size = VOCAB_SIZE,\n",
    "    trg_vocab_size = VOCAB_SIZE,\n",
    "    src_pad_idx = VOCAB['<PAD>'],\n",
    "    trg_pad_idx = VOCAB['<PAD>'],\n",
    "    emb_size = 512,\n",
    "    n_layers=1,\n",
    "    heads=4,\n",
    "    forward_expansion=4,\n",
    "    drop_out=0.1,\n",
    "    max_seq_len=TARGET_SEQ_LEN,\n",
    "    device=device\n",
    ").to(device)\n",
    "\n",
    "loss_function = torch.nn.CrossEntropyLoss(ignore_index=VOCAB['<PAD>'], reduction='mean')\n",
    "optimizer = optim.Adam(pytorch_transformer.parameters())\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "\n",
    "pytorch_transformer = train(pytorch_transformer, dataloader, optimizer, loss_function, clip=1, epochs=100, VOCAB=VOCAB, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d46b4e61",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-03T05:18:05.627357Z",
     "iopub.status.busy": "2024-02-03T05:18:05.626718Z",
     "iopub.status.idle": "2024-02-03T05:18:05.911108Z",
     "shell.execute_reply": "2024-02-03T05:18:05.910187Z"
    },
    "papermill": {
     "duration": 0.313708,
     "end_time": "2024-02-03T05:18:05.915338",
     "exception": false,
     "start_time": "2024-02-03T05:18:05.601630",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "question:  can i make cron start a process at say at night and stop it at say in the morning\n",
      "generated_answers:  make a script to start the app and chave cron call that at then another sc a\n",
      "real answer:  make a script to start the app and chave cron call that at then another sc\n",
      "\n",
      "question:  how do i configure vpn\n",
      "generated_answers:  it is not difficult to update to copy it should be updategrub to the kernel it is in the next\n",
      "real answer:  it is focused on the settings for the vyprvpn service so just ignore the settings it requires for that\n",
      "\n",
      "question:  hey can someone tell me how to install a firefox theme that is in a jar\n",
      "generated_answers:  open firefox go to tools themes menu and drag and drop the jar file onto the list of themes can\n",
      "real answer:  open firefox go to tools themes menu and drag and drop the jar file onto the list of themes\n",
      "\n",
      "question:  anyone ever had an issue with evolution not grabbing pop email via gmail\n",
      "generated_answers:  yes tough did not found a solution was not able to access it for a few days yes\n",
      "real answer:  yes tough did not found a solution was not able to access it for a few days\n",
      "\n",
      "question:  what is envy\n",
      "generated_answers:  i think you can just a kernel panics ie blinking keyboard lights no specific bug reports of the filesystem you\n",
      "real answer:  a script that tries to install your drivers we reworked a version of it for your repos but the\n",
      "\n",
      "question:  is anyone here using networkmanager with wlan and static ip here now i would like to know how nmapplets\n",
      "generated_answers:  at least static ip seems to be fixed for ethernet in karmic but i dont use wifi you\n",
      "real answer:  at least static ip seems to be fixed for ethernet in karmic but i dont use wifi\n",
      "\n",
      "question:  what is the fastest and easiest way to crack wep in ubuntu i am using it to show my\n",
      "generated_answers:  aircrackng would be one that can do it but i have never used it and do not know how it\n",
      "real answer:  aircrackng would be one that can do it but i have never used it and do not know how\n",
      "\n",
      "question:  hi i am using ubuntu it was suported until april when i run updatemanager d it show a button\n",
      "generated_answers:  the best thing to do is upgrade sequentially or you might want to install over what you have it is\n",
      "real answer:  the best thing to do is upgrade sequentially or you might want to install over what you have it\n",
      "\n",
      "question:  is sata generally hotswappable i bought an enclosure that supports esata and it came with a bracket that just\n",
      "generated_answers:  at least as far as i know there is no linux support for hotswapping esata yet you\n",
      "real answer:  at least as far as i know there is no linux support for hotswapping esata yet\n",
      "\n",
      "question:  since i upgraded to i have not been able to run flash\n",
      "generated_answers:  you can just install ubunturestrictedextras and get a bunch of other needed codecs too make sure you removed any a\n",
      "real answer:  you can just install ubunturestrictedextras and get a bunch of other needed codecs too make sure you removed any\n",
      "\n",
      "question:  is sunbird or qorganizer a good choice for a non email client calendar that will pop up and remind\n",
      "generated_answers:  the program will have to either be running in the background or have some sort of trigger tied to be\n",
      "real answer:  the program will have to either be running in the background or have some sort of trigger tied to\n",
      "\n",
      "question:  hi everyone i am gonna get off topic for a second when i download a torrent i get like\n",
      "generated_answers:  just burn the file that was playing the movie on your computer nothing more or less less\n",
      "real answer:  just burn the file that was playing the movie on your computer nothing more or less\n",
      "\n",
      "question:  when a friend of mine unplugs his power adapter from his laptop it goes into standby any ideas\n",
      "generated_answers:  research apm automatic power management it messys with my battery too and i am running a desktop research\n",
      "real answer:  research apm automatic power management it messys with my battery too and i am running a desktop\n",
      "\n",
      "question:  i am running the rc on this laptop how can i have it updated so it is at the\n",
      "generated_answers:  you probably already are up to level there is no specific difference with rc if rc is uptodate probably\n",
      "real answer:  you probably already are up to level there is no specific difference with rc if rc is uptodate\n",
      "\n",
      "question:  anyone know why i get playback device not found when trying to play midi\n",
      "generated_answers:  afaik it is because midi is not set up by default on ubuntu go searching the wikiforums is\n",
      "real answer:  afaik it is because midi is not set up by default on ubuntu go searching the wikiforums\n",
      "\n",
      "question:  is there a way to get the fingerprint of a gpg public key without using the stupid gui\n",
      "generated_answers:  email or just listkeys and then peruse them if you just added it it will have todays date it down\n",
      "real answer:  email or just listkeys and then peruse them if you just added it it will have todays date it\n",
      "\n",
      "question:  is there a way to take an xubuntu install and turn it into a regular install of ubuntu gnome\n",
      "generated_answers:  also theres an aptget command that cleans out unused packages it is possible you have an unbelievable amount of packages\n",
      "real answer:  also theres an aptget command that cleans out unused packages it is possible you have an unbelievable amount of\n",
      "\n",
      "question:  can i just copy a livecd to my filesystem to install or do i need to do more\n",
      "generated_answers:  you can use special tools to put the live cd direct onto a us disk so i dont see if\n",
      "real answer:  you can use special tools to put the live cd direct onto a us disk so i dont see\n",
      "\n",
      "question:  should i install skype from canonical partner or should i install it directly from skypecom\n",
      "generated_answers:  the last update was ages ago but if you install it from the repository you will get update in the\n",
      "real answer:  the last update was ages ago but if you install it from the repository you will get update in\n",
      "\n",
      "question:  hi all how can i turn off the notifications eg when somebody goes online in pidgin etc at the\n",
      "generated_answers:  right click the pidgin icon and select preferences see if what you want to do is in there see\n",
      "real answer:  right click the pidgin icon and select preferences see if what you want to do is in there\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/transformer.py:287: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/NestedTensorImpl.cpp:177.)\n",
      "  output = torch._nested_tensor_from_mask(output, src_key_padding_mask.logical_not(), mask_check=False)\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/activation.py:1144: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/native/transformers/attention.cpp:150.)\n",
      "  return torch._native_multi_head_attention(\n"
     ]
    }
   ],
   "source": [
    "def test_model(model, enc_src, dec_src, trg, VOCAB, max_length=15):\n",
    "    enc_src = enc_src.to(device)\n",
    "#     dec_src = dec_src.to(device)\n",
    "    trg = trg.to(device)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    generated_answers = torch.LongTensor().to(device)\n",
    "    batch_size, seq_len = dec_src.shape\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        initial_dec_src = dec_src  # Shape: (batch_size, seq_len)\n",
    "\n",
    "        dec_src = torch.LongTensor([VOCAB['<SOS>']]).unsqueeze(0).repeat(batch_size, 1).to(device)\n",
    "#         dec_src = torch.LongTensor(dec_src[:, :2]).to(device)\n",
    "\n",
    "        for i in range(max_length):\n",
    "            logits = model(enc_src, dec_src)\n",
    "            predictions = torch.argmax(logits, dim=2)\n",
    "            \n",
    "            # Append the last prediction\n",
    "            dec_src = torch.cat((dec_src, predictions[:, i].unsqueeze(1)), dim=1)\n",
    "\n",
    "            generated_answers = torch.cat((generated_answers, predictions[:, i].unsqueeze(1)), dim=1)\n",
    "\n",
    "    \n",
    "    for i in range(max_length):\n",
    "        print('\\nquestion: ', tokens_to_text(enc_src[i]))\n",
    "        print('generated_answers: ', tokens_to_text(generated_answers[i]))\n",
    "        print('real answer: ', tokens_to_text(trg[i]))\n",
    "            \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "batch = next(iter(dataloader))\n",
    "enc_src, dec_src, trg = batch  # Ensure that these tensors are correctly shaped as per model requirements\n",
    "\n",
    "# test_model(transformer, enc_src, dec_src, trg, VOCAB, max_length=TARGET_SEQ_LEN)\n",
    "test_model(pytorch_transformer, enc_src, dec_src, trg, VOCAB, max_length=TARGET_SEQ_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "22e40aca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-03T05:18:05.965548Z",
     "iopub.status.busy": "2024-02-03T05:18:05.965176Z",
     "iopub.status.idle": "2024-02-03T05:18:06.327745Z",
     "shell.execute_reply": "2024-02-03T05:18:06.326856Z"
    },
    "papermill": {
     "duration": 0.389256,
     "end_time": "2024-02-03T05:18:06.329933",
     "exception": false,
     "start_time": "2024-02-03T05:18:05.940677",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: hi i just tried to boot off the ubuntu livecd on my laptop but it fails in the initrd\n",
      "Real Answer: try going to your bios setting and turning on legacy emulation for your disc drives etc\n",
      "\n",
      "Scratch Transformer Response: to it to to it it it it it it it it it it it it now it it it\n",
      "\n",
      "Pytorch Transformer Response: try going to your bios setting and turning on legacy emulation for your disc drives etc\n",
      "\n",
      "Transformer Response 2: i also do not the same place but i do not see the legacy your system and got\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def prepare_model_input(question, max_length=50):\n",
    "    # Tokenize the input question\n",
    "    tokenized_question = text_to_tokens(question)\n",
    "    enc_src = tokenized_question + [VOCAB['<EOS>']]  # Add SOS and EOS tokens\n",
    "    \n",
    "    # Prepare a placeholder for the decoder's input\n",
    "    dec_src = torch.LongTensor([VOCAB['<SOS>']]).unsqueeze(0).to(device)\n",
    "\n",
    "    # Convert to tensor and add batch dimension\n",
    "    enc_src = F.pad(torch.LongTensor(enc_src), (0, max_length - len(enc_src)), mode='constant', value=VOCAB['<PAD>']).unsqueeze(0).to(device)\n",
    "\n",
    "    return enc_src, dec_src\n",
    "\n",
    "\n",
    "def chat_with_transformer(model, question, max_length=50, temperature=1):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        enc_src, dec_src = prepare_model_input(question, max_length=max_length)\n",
    "        \n",
    "        # Placeholder for the generated answer\n",
    "        generated_answer = []\n",
    "        for i in range(max_length):\n",
    "            # Forward pass through the model\n",
    "            logits = model(enc_src, dec_src)\n",
    "\n",
    "            # Get the token with the highest probability for the next position from the last time step\n",
    "            predictions = F.softmax(logits / temperature, dim=2)[:, i, :]\n",
    "            predicted_token = torch.multinomial(predictions, num_samples=1).squeeze(1)\n",
    "\n",
    "            # Break if the EOS token is predicted\n",
    "            if predicted_token.item() == VOCAB['<EOS>']:\n",
    "                break\n",
    "\n",
    "            # Append the predicted token to the decoder's input for the next time step\n",
    "            dec_src = torch.cat((dec_src, predicted_token.unsqueeze(0)), dim=1)\n",
    "\n",
    "            # Append the predicted token to the generated answer\n",
    "            generated_answer.append(predicted_token.item())\n",
    "\n",
    "        # Convert the generated tokens to words\n",
    "        return tokens_to_text(generated_answer)\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "batch = next(iter(dataloader))\n",
    "dec_src, enc_src, trg = batch\n",
    "\n",
    "question = tokens_to_text(dec_src[0].tolist())\n",
    "answer = tokens_to_text(trg[0].tolist())\n",
    "print('Question:', question)\n",
    "print('Real Answer:', answer)\n",
    "\n",
    "transformer_response = chat_with_transformer(transformer, question, max_length=TARGET_SEQ_LEN, temperature=1.0)\n",
    "print('\\nScratch Transformer Response:', transformer_response)\n",
    "\n",
    "\n",
    "transformer_response = chat_with_transformer(pytorch_transformer, question, max_length=TARGET_SEQ_LEN, temperature=1.0)\n",
    "print('\\nPytorch Transformer Response:', transformer_response)\n",
    "\n",
    "\n",
    "question = 'Do you have friends?'\n",
    "transformer_response = chat_with_transformer(pytorch_transformer, question, max_length=TARGET_SEQ_LEN, temperature=1.0)\n",
    "print('\\nTransformer Response 2:', transformer_response)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 13433,
     "sourceId": 18178,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4392216,
     "sourceId": 7542499,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4392276,
     "sourceId": 7542594,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4392299,
     "sourceId": 7542652,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30588,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 255.849348,
   "end_time": "2024-02-03T05:18:08.696849",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-02-03T05:13:52.847501",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
